{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关注的模型\n",
    "\n",
    "- **金融交易**：在这里，环境指整个金融市场和所有影响它的事物。这涵盖非常多的事情，例如最近的新闻、经济和政治状况、天气、食物供应和推特趋势。甚至你今天决定待在家里的决定也可能会间接影响世界的金融系统（如果你相信“蝴蝶效应”的话）。然而，我们的观察仅限于股票价格、新闻等。我们无法查看环境中的大部分状态（是它们使金融交易变得如此复杂）。\n",
    "- **电脑游戏**：这里的环境是你的计算机的状态，包括所有内存和磁盘数据。对于网络游戏来说，还包括其他计算机以及它们与你的计算机之间的所有互联网基础设施。观察则只是屏幕中的像素和声音。这些像素信息并不是小数量级的（有人计算过，取中等大小图片（1024×768）的像素进行排列组合，其可能结果的数量明显大于我们星系中的原子数量），但整个环境的状态的数量级肯定更大。\n",
    "- **网页浏览**：这里的环境是互联网，包括我们的工作计算机和服务器计算机之间的所有网络基础设施，包含了数百万个不同的组件。观察则是当前浏览步骤中加载的网页。NN结构搜索：在本例中，环境相当简单，包括执行特定NN评估的工具集和用于获取性能指标的数据集。与互联网相比，它就像个玩具环境。观察则不同，它包括关于测试的一些信息，例如损失函数的收敛状态或者能从评估步骤中获得的其他指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习方法\n",
    "\n",
    "## 分类\n",
    "\n",
    "- 使用什么参数进行训练\n",
    "    - 无模型：直接通过观察确定的动作训练智能体。\n",
    "    - 有模型：需要观察动作在模型中的表现来训练。\n",
    "- 需要得出什么返回值\n",
    "    - 基于策略：智能体直接给出动作概率。\n",
    "    - 基于价值：通过计算所有动作可能产生的价值决定动作。\n",
    "- 从何处取得训练数据训练\n",
    "    - 在线策略\n",
    "    - 离线策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 交叉熵\n",
    "\n",
    "但简单且功能强大\n",
    "\n",
    "## 缺陷\n",
    "\n",
    "对于训练来说，片段必须是有限的、优秀的、简短的。\n",
    "片段的总奖励应该有足够的差异来区分好的片段和差的片段。\n",
    "没有中间值来表明智能体成功了还是失败了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "\n",
    "# Bellman方程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
